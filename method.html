<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Method</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="./index" class="logo">Super Resolution</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li><a href="./index">Abstract</a></li>
							<li><a href="./intro">Introduction</a></li>
							<li><a href="./dataset">Dataset</a></li>
							<li class="active"><a href="./method">Method</a></li>
							<li><a href="./image_preprocessing">Preprocessing</a></li>
							<li><a href="./evaluation">Evaluation</a></li>
							<li><a href="./conclusion">Conclusion</a></li>
							<li><a href="./demo">Demo</a></li>
						</ul>
						<ul class="icons">
							<li><a href="https://github.com/rushk99/Image-Super-Resolution-SRGAN" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								<header class="major">
									<h1>Method</h1>
								</header>
		
								<p>In Single Image Super Resolution (SISR) the aim is to estimate a high-resolution, superresolved image 
									SR from a low-resolution input image
									
									LR. Here 
									LR is the low-resolution version of its highresolution counterpart 
									HR. The high-resolution images
									are only available during training. In training, 
									LR is
									obtained by performing downsampling operations on HR using bicubic interpolation with a factor of 4.
									For an image with C color channels, we describe 
									LR by a real-valued tensor of size W × H × C and HR, SR by
									4xW × 4xH × C respectively.
									Our ultimate goal is to train a generating function G that
									estimates for a given LR input image its corresponding HR
									counterpart.
									<br>
									We are training two models, one is the generator and other is the discriminator. Generator is trained to minimize
									the loss(i.e. to generate photo-realistic images) and Discriminator is trained to maximize the loss(i.e. to distinguish between generated images and original images).
									In other words, D and G play the following two-player minimax game.
									</p>
									<div class="image main"><img src="images/min_max.png" alt="" /></div>
							</section>
							<section class="post">
								<header class="major">
									<h1>Loss Function</h1>
								</header>
								<p>Loss Function:

									The SRGAN uses perpetual loss function (LSR)  which is the weighted sum of two loss components : content loss and adversarial loss. This loss is very important for the performance of the generator architecture:
									
									Content Loss: We use two types of content loss in this paper : pixelwise MSE loss for the SRResnet architecture, which is most common MSE loss for image Super Resolution. However MSE loss does not able to deal with high frequency content in the image that resulted in producing overly smooth images. Therefore the authors of the paper decided to  use loss of different VGG layers. This VGG loss is based on the ReLU activation layers of the pre-trained 16 layer VGG network. This loss is defined as follows:
									<div class="image main"><img src="images/simplecontentloss.png" alt="" /></div>
									<p style="text-align:center">
										Simple Content Loss
										</p>
									<div class="image main"><img src="images/vggcontentloss.png" alt="" /></div>
									<p style="text-align:center">
										VGG content loss
										</p>
									
									Adversarial Loss: The Adversarial loss is the loss function that forces the generator to image more similar to high resolution image by using a discriminator that is trained to differentiate between high resolution and super resolution images.
									
									<div class="image main"><img src="images/adversarial_loss.png" alt="" /></div>
									<p style="text-align:center">
										Adversarial Loss
										</p>
									Therefore total content loss of this architecture will be :
									<div class="image main"><img src="images/generative loss.png" alt="" /></div>
									<p style="text-align:center">
										Total Content Loss
										</p>
									</p>
							</section>


							<section class="post">
								<header class="major">
									<h1>Architecture</h1>
								</header>
								<p>Generator Architecture:

									The generator architecture contains residual network instead of deep convolution networks because residual networks are easy to train and allows them to be substantially deeper in order to generate better results. This is because the residual network used a type of connections called skip connections.
									
									
									
									There are B residual blocks (16), originated by ResNet. Within the residual block, two convolutional layers are used, with small 3×3 kernels and 64 feature maps followed by batch-normalization layers and ParametricReLU as the activation function.
									
									The resolution of the input image is increased with two trained sub-pixel convolution layers.
									
									This generator architecture also uses parametric ReLU as an activation function which instead of using a fixed value for a parameter of the rectifier (alpha) like LeakyReLU. It adaptively learns the parameters of rectifier and   improves the accuracy at negligible extra computational cost
									
									  During the training, A high-resolution image (HR) is downsampled to a low-resolution image (LR). The generator architecture than tries to upsample the image from low resolution to super-resolution. After then the image is passed into the discriminator, the discriminator and tries to distinguish between a super-resolution and High-Resolution image and generate the adversarial loss which then backpropagated into the generator architecture.</p>
								<div class="image main"><img src="images/generator.png" alt="" /></div>
								<p>Discriminator Architecture: 
									
									The task of the discriminator is to discriminate between real HR images and generated SR images.   The discriminator architecture used in this paper is similar to DC- GAN architecture with LeakyReLU as activation. The network contains eight convolutional layers with of 3×3 filter kernels, increasing by a factor of 2 from 64 to 512 kernels. Strided convolutions are used to reduce the image resolution each time the number of features is doubled. The resulting 512 feature maps are followed by two dense layers and a leakyReLU applied between and a final sigmoid activation function to obtain a probability for sample classification. 
								</p>
								<div class="image main"><img src="images/discriminator.png" alt="" /></div>
							</section>

					</div>
			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>